#  365 Days Challenge ‚Äî AI √ó Astrophysics 

> ‚ÄúSomewhere, something incredible is waiting to be known.‚Äù ‚Äî Carl Sagan  

Welcome to my **365 Days of GitHub Challenge**, where I‚Äôll combine my passion for **Machine Learning** ü§ñ and **Astrophysics** üå† to learn, build, and share projects every single day for one year!

---

##  Challenge Goal

-  Learn and apply **AI/ML concepts** daily  
-  Explore **real astrophysics datasets** (NASA, ESA, etc.)  
-  Build, document, and publish **projects** on GitHub  
-  Maintain **1 commit per day** for 365 days  
-  Share progress and inspire others to learn AI for space!

**Start Date:** November 8, 2025  
**End Date:** November 7, 2026  

---

##  Yearly Roadmap (12-Month Plan)

| Month | Theme | Focus | Status |
|-------|--------|--------|--------|
| **Nov 2025** |  Foundations | Python, Git, ML basics | üü¢ Ongoing |
| **Dec 2025** |  Core ML Projects | Classification, clustering | ‚¨ú Pending |
| **Jan 2026** |  Deep Learning | CNNs, PyTorch | ‚¨ú Pending |
| **Feb 2026** |  Astronomy Data 101 | FITS files, AstroPy, AstroML | ‚¨ú Pending |
| **Mar 2026** |  Galaxy Classifier | Galaxy Zoo, CNN | ‚¨ú Pending |
| **Apr 2026** |  Exoplanet & Signal ML | Kepler data, time series | ‚¨ú Pending |
| **May 2026** |  Solar Flare Prediction | LSTM, forecasting | ‚¨ú Pending |
| **Jun 2026** |  Generative Space | GANs, diffusion models | ‚¨ú Pending |
| **Jul 2026** |  AI for Physics Simulations | N-body & particle data | ‚¨ú Pending |
| **Aug 2026** |  Open Source Month | Contribute to AstroML / Astropy | ‚¨ú Pending |
| **Sep 2026** |  Research + Visualization | Space analytics dashboards | ‚¨ú Pending |
| **Oct 2026** |  Capstone Build | AI-Powered Astronomy Explorer | ‚¨ú Pending |

---

##  Monthly Focus (Example Breakdown)

###  Month 3: Galaxy Classification Project
**Goal:** Build a CNN to classify galaxies (spiral, elliptical, irregular).  
**Dataset:** [Galaxy Zoo](https://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge/data)  
**Tools:** PyTorch, AstroPy, Matplotlib  
**Tasks:**
- Week 1: Load & clean dataset  
- Week 2: Train CNN  
- Week 3: Tune hyperparameters  
- Week 4: Visualize & deploy results  

---

##  Daily Log

| Day | Date | Activity | Notes |
|-----|------|-----------|-------|
| 1 | Nov 8, 2025 | Initialized repo, created README | Set up challenge plan |
| 2 | Nov 9, 2025 | Reviewed Python and Git basics | Installed AstroPy, TensorFlow |
| 3 | Nov 10, 2025 | Practiced linear regression | Started simple ML notebooks |
| 4 | Nov 11, 2025 | Visualized galaxy dataset | Used Matplotlib & Pandas |
| ... | ... | ... | ... |


---

##  Tools & Libraries

- **Languages:** Python  
- **Frameworks:** PyTorch, TensorFlow, Scikit-learn  
- **Astronomy Tools:** AstroPy, AstroML, FITS I/O  
- **Visualization:** Matplotlib, Plotly, Seaborn  
- **Dev Tools:** Git, GitHub, Jupyter, VS Code  

---

##  Progress Tracker

| Milestone | Goal | Status |
|------------|------|--------|
|  30 Days | Complete daily commits for 1 month | ‚¨ú |
|  100 Days | Finish 3 AI mini-projects | ‚¨ú |
|  200 Days | Publish 1 AstroML project | ‚¨ú |
|  300 Days | Contribute to open source | ‚¨ú |
|  365 Days | Complete Capstone & Reflect | ‚¨ú |

---

## üß≠ Connect & Collaborate

If you‚Äôre also into **AI + Space**, feel free to collaborate or share ideas!  
üåê **GitHub:** [Kritimbist](https://github.com/Kritimbist)  
  
---

### ‚≠ê Support

If you like this challenge or find it inspiring, **star this repo** ‚≠ê to support the journey!








## Day 001 : Data Distribution using python

Today i learn about the concept of data distribution and some of it's examples .  Data distribution show how data values are spread out and it helps in :

- Detect outliers
- See if your data is balanced or skewed
- Decide whether you need normalization or transformation
- Detect imbalanced classes in classification problems

I've use  python module Numpy, where we can create random data sets of any size.
# some of the examples  of data distribution

<img width="964" height="770" alt="image" src="https://github.com/user-attachments/assets/57749f77-cf2c-46f3-b43a-44510f7004f1" />




## Day 002 : Visualizing Data Distribution using python

Today i learn about how to visualize the distributed data . 


## üìà Visualization Types


###  Histogram
A **histogram** shows how data is distributed by grouping values into bins.
We will use the Python module Matplotlib to draw a histogram.
# sample example of histogram:
<img width="846" height="702" alt="image" src="https://github.com/user-attachments/assets/ccce542b-ba96-441a-b9a3-e128832cc591" />



##  Normal Distribution 

A **normal distribution** is characterized by:
- A **mean (Œº)** ‚Äî the center of the curve  
- A **standard deviation (œÉ)** ‚Äî controls how spread out the data is  

The formula for the probability density function is:

A Probability Density Function (PDF) is a mathematical function that tells you how likely a continuous outcome (like height, time, or temperature) is to fall within a certain range.

$$
f(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}
$$

# Simple example of visualizing noram distribution:
<img width="762" height="689" alt="image" src="https://github.com/user-attachments/assets/cfdae9be-9ef1-4375-934f-958fc60c7149" />



##  scatter plot
A scatter plot is a diagram where each value in the data set is represented by a dot.
<img width="640" height="480" alt="image" src="https://github.com/user-attachments/assets/168d08ea-cf6f-43cb-aa75-625004600152" />

The Matplotlib module has a method for drawing scatter plots, it needs two arrays of the same length, one for the values of the x-axis, and one for the values of the y-axis:
- x = [5,7,8,7,2,17,2,9,4,11,12,9,6]
- y = [99,86,87,88,111,86,103,87,94,78,77,85,86]

Use the scatter() method to draw a scatter plot diagram.

# Simple example of scatter plot :

<img width="907" height="726" alt="image" src="https://github.com/user-attachments/assets/c2e18951-9daf-44a9-8334-456b80893b05" />

## Random Data Distributions
In Machine Learning the data sets can contain thousands-, or even millions, of values.
You might not have real world data when you are testing an algorithm, you might have to use randomly generated values.

Let us create two arrays that are both filled with 1000 random numbers from a normal data distribution.
The first array will have the mean set to 5.0 with a standard deviation of 1.0.
The second array will have the mean set to 10.0 with a standard deviation of 2.0:

simple examples of random data distribution :
<img width="904" height="705" alt="image" src="https://github.com/user-attachments/assets/eb810ce1-a45e-417d-a215-b7a58628f7ad" />

# Regression

In machine learning, regression is a type of supervised learning technique used to predict a continuous numerical value.
Think of it as finding the mathematical relationship between a set of input features (like the size of a house, number of bedrooms, and location) and a continuous output (like its final sale price). The goal is to build a model that can take new, unseen input features and make an accurate prediction about the output.

Supervised learning is a type of machine learning where the model learns from labeled data ‚Äî meaning we already know the input and the correct output.
You can think of it like a teacher guiding a student :
- The teacher gives examples (inputs) and the correct answers (labels).
- The student (model) learns the pattern between them.
- Later, the student can predict answers for new examples.
  
Continuous numerical values are numbers that can take any value within a range ‚Äî including fractions and decimals.

Think of them as values that change smoothly instead of jumping from one value to another.

üîπ Examples:

Height ‚Üí 165.4 cm, 170.2 cm, 171.8 cm

Weight ‚Üí 55.5 kg, 55.55 kg, 56.1 kg

Temperature ‚Üí 25.1¬∞C, 25.15¬∞C, 25.2¬∞C

Time ‚Üí 2.5 seconds, 2.55 seconds, 2.556 seconds

These can have infinite possible values between any two numbers.

<img width="2048" height="1939" alt="image" src="https://github.com/user-attachments/assets/2ff7031f-6d7d-4c4d-9e07-2f289cafdf89" />

## üìà Common Types of Regression Algorithms
While all regression models aim to predict a number, they use different methods to find the underlying patterns in the data.

### Linear Regression
This is the simplest and most common type. It assumes a linear relationship (a straight line) between the input variables (X) and the output variable (Y).


# Simple Linear Regression
This is used when you have one independent variable (X) to predict a continuous dependent variable (Y). The goal is to find the best-fitting straight line for the data.The formula for the regression line is:
$$
y = b_0 + b_1 * x + \epsilon
$$

Where:  
- \( y \) = Dependent variable (what we want to predict)  
- \( x \) = Independent variable (input feature)  
- \( b_0 \) = Intercept (value of \( y \) when \( x = 0 \))  
- \( b_1 \) = Slope (how much \( y \) changes with one unit increase in \( x \))  
- \( \epsilon \) = Error term (difference between actual and predicted value)

<img width="3000" height="2000" alt="image" src="https://github.com/user-attachments/assets/efc78c04-9f64-494d-beb9-7c4eaeb0bfd4" />


# Simple example for simple linear regression :
<img width="1215" height="704" alt="image" src="https://github.com/user-attachments/assets/6be23bcf-e22e-4bbd-8b2a-fc76eedb906b" /> <img width="1052" height="768" alt="image" src="https://github.com/user-attachments/assets/398aab09-a5c9-458c-8901-42342a50d426" />
<img width="963" height="710" alt="image" src="https://github.com/user-attachments/assets/ac13fc52-9309-497b-b0d6-5084d1948321" />

##  Code Breakdown: 
- We create two numpy arrays. X is our input feature, and y is our target output.Important: scikit-learn models expect the X data to be a 2D array, even for simple regression with one feature. X.reshape(-1, 1) converts our 1D array [1, 2, 3...] into a 2D array [[1], [2], [3]...].
- Create and Train the Model:model = LinearRegression(): We create an instance of the regression model.model.fit(X, y): This is the "training" step. The model "learns" the best possible straight line to fit the X and y data points by finding the optimal values for the slope ($b_1$) and intercept ($b_0$).
- View the Results:model.intercept_: This gives you the calculated intercept ($b_0$) from the formula.model.coef_: This gives you an array of coefficients. Since this is simple linear regression, we only have one, coef_[0], which is our slope ($b_1$).
- Make Predictions:model.predict(X): We use the trained model to predict the y-values for our original X-values. This gives us the points that lie directly on the red line.
- Plot the Results:plt.scatter(X, y): This plots our original, "real" data points as blue dots.plt.plot(X, y_pred): This plots the predicted values as a red line, which is the final regression line.











